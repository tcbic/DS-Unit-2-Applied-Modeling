{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mLjlSwVGRli8"
   },
   "source": [
    "_Lambda School Data Science â€” Applied Modeling_ \n",
    "\n",
    "This sprint, your project is Caterpillar Tube Pricing: Predict the prices suppliers will quote for industrial tube assemblies.\n",
    "\n",
    "# Gradient Boosting\n",
    "\n",
    "\n",
    "#### Objectives\n",
    "- Do feature engineering with relational data\n",
    "- Use xgboost for gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lxo6A73ERli-"
   },
   "source": [
    "#### Python libraries for Gradient Boosting\n",
    "- [scikit-learn Gradient Tree Boosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting) â€” slower than other libraries, but [the new version may be better](https://twitter.com/amuellerml/status/1129443826945396737)\n",
    "  - Anaconda: already installed\n",
    "  - Google Colab: already installed\n",
    "- [xgboost](https://xgboost.readthedocs.io/en/latest/) â€”Â can accept missing values and enforce [monotonic constraints](https://xiaoxiaowang87.github.io/monotonicity_constraint/)\n",
    "  - Anaconda, Mac/Linux: `conda install -c conda-forge xgboost`\n",
    "  - Windows: `conda install -c anaconda py-xgboost`\n",
    "  - Google Colab: already installed\n",
    "- [LightGBM](https://lightgbm.readthedocs.io/en/latest/) â€”Â can accept missing values and enforce [monotonic constraints](https://blog.datadive.net/monotonicity-constraints-in-machine-learning/)\n",
    "  - Anaconda: `conda install -c conda-forge lightgbm`\n",
    "  - Google Colab: already installed\n",
    "- [CatBoost](https://catboost.ai/) â€”Â can accept missing values and use [categorical features](https://catboost.ai/docs/concepts/algorithm-main-stages_cat-to-numberic.html) without preprocessing\n",
    "  - Anaconda: `conda install -c conda-forge catboost`\n",
    "  - Google Colab: `pip install catboost`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S3mx5xMnRli_"
   },
   "source": [
    "### Get data\n",
    "\n",
    "\n",
    "#### Option 1. Kaggle web UI\n",
    " \n",
    "Sign in to Kaggle and go to the [Caterpillar Tube Pricing](https://www.kaggle.com/c/caterpillar-tube-pricing) competition. Go to the Data page. After you have accepted the rules of the competition, use the download buttons to download the data.\n",
    "\n",
    "\n",
    "#### Option 2. Kaggle API\n",
    "\n",
    "Follow these [instructions](https://github.com/Kaggle/kaggle-api).\n",
    "\n",
    "#### Option 3. GitHub Repo â€” LOCAL\n",
    "\n",
    "If you are working locally:\n",
    "\n",
    "1. Clone the [GitHub repo](https://github.com/LambdaSchool/DS-Unit-2-Applied-Modeling/tree/master/data/caterpillar) locally. The data is in the repo, so you don't need to download it separately.\n",
    "\n",
    "2. Unzip the file `caterpillar-tube-pricing.zip` which is in the data folder of your local repo.\n",
    "\n",
    "3. Unzip the file `data.zip`. \n",
    "\n",
    "4. Run the cell below to assign a constant named `SOURCE`, a string that points to the location of the data on your local machine. The rest of the code in the notebook will use this constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4llRWHx4EI2q"
   },
   "outputs": [],
   "source": [
    "SOURCE = '../data/caterpillar/caterpillar-tube-pricing/competition_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vvyyeP90FB65"
   },
   "source": [
    "#### Option 4. GitHub Repo â€” COLAB\n",
    "\n",
    "If you are working on Google Colab, uncomment and run these cells, to download the data, unzip it, and assign a constant that points to the location of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "vzVWh6oGFZJb",
    "outputId": "33cbef79-e9d0-4079-fb7a-9730a200a169"
   },
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Applied-Modeling/master/data/caterpillar/caterpillar-tube-pricing.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "1QG9BiopRljC",
    "outputId": "3bf33982-29bc-464b-9aa1-8f4698e403c3"
   },
   "outputs": [],
   "source": [
    "# !unzip caterpillar-tube-pricing.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "67Pz81FKRljE",
    "outputId": "74d3f6a0-466a-4157-b229-ad459015f20a"
   },
   "outputs": [],
   "source": [
    "# !unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mF8uDQ5wFSny"
   },
   "outputs": [],
   "source": [
    "# SOURCE = 'competition_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6_ZDsGjVRljF"
   },
   "source": [
    "## Do feature engineering with relational data\n",
    "\n",
    "Here are some questions â€”Â not answers!\n",
    "\n",
    "### `bill_of_materials`\n",
    "\n",
    "is formatted like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "opLig3sDRljG",
    "outputId": "fc74376a-bb66-494c-a42a-899cefae7cac"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "materials = pd.read_csv(SOURCE + 'bill_of_materials.csv')\n",
    "materials.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7_nd_BN1RljI"
   },
   "source": [
    "#### Would this be a better representation?\n",
    "\n",
    "Could pandas melt, crosstab, and other functions help reshape the data like this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DKgBu-T2RljI"
   },
   "source": [
    "| Crosstab | C-1622 | C-1629 | C-1312 | C-1624 | C-1631 | C-1641 | Distinct | Total |\n",
    "|:--------:|:------:|--------|--------|--------|--------|--------|----------|-------|\n",
    "| TA-00001 | 2      | 2      | 0      | 0      | 0      | 0      | 2        | 4     |\n",
    "| TA-00002 | 0      | 0      | 2      | 0      | 0      | 0      | 1        | 2     |\n",
    "| TA-00003 | 0      | 0      | 2      | 0      | 0      | 0      | 1        | 2     |\n",
    "| TA-00004 | 0      | 0      | 2      | 0      | 0      | 0      | 1        | 2     |\n",
    "| TA-00005 | 0      | 0      | 0      | 1      | 1      | 1      | 3        | 3     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5fBGv8CIRljJ"
   },
   "source": [
    "### `components`\n",
    "\n",
    "Contains three representations of each component, in order of decreasing cardinality / granularity:\n",
    "\n",
    "- `component_id`\n",
    "- `name`\n",
    "- `component_type_id`\n",
    "\n",
    "What are the pros & cons of these different representations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "gkj_leYyRljJ",
    "outputId": "4b775906-efab-47f7-988a-9f888c1d2070"
   },
   "outputs": [],
   "source": [
    "components = pd.read_csv(SOURCE + 'components.csv')\n",
    "components.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "cNowF5PvV1wH",
    "outputId": "e90a22d4-2ba2-44d9-9589-20f55ff16df9"
   },
   "outputs": [],
   "source": [
    "components.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZGzxHS-yId5U"
   },
   "source": [
    "### Tip/trick: Want to read all the files at once?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QuektlLMY-hu"
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "data = {}\n",
    "for path in glob(SOURCE + '*.csv'):\n",
    "    df = pd.read_csv(path)\n",
    "    filename = path.split('/')[-1]\n",
    "    name = filename.split('.')[0]\n",
    "    data[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NspIenHYRljL"
   },
   "source": [
    "## Example solution for last assignment ðŸšœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "nxhNfQLlRljL",
    "outputId": "3c0691b6-73ec-4ca1-db25-4416dff8b9b1"
   },
   "outputs": [],
   "source": [
    "# !pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sJB9x5GcRljN",
    "outputId": "ee256c81-ebc5-4211-8a5c-f7bd51d00778"
   },
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "def wrangle(X):\n",
    "    X = X.copy()\n",
    "    \n",
    "    # Engineer date features\n",
    "    X['quote_date'] = pd.to_datetime(X['quote_date'], infer_datetime_format=True)\n",
    "    X['quote_date_year'] = X['quote_date'].dt.year\n",
    "    X['quote_date_month'] = X['quote_date'].dt.month\n",
    "    X = X.drop(columns='quote_date')\n",
    "    \n",
    "    # Merge tube data\n",
    "    tube = pd.read_csv(SOURCE + 'tube.csv')\n",
    "    X = X.merge(tube, how='left')\n",
    "    \n",
    "    # Engineer features from bill_of_materials\n",
    "    materials = pd.read_csv(SOURCE + 'bill_of_materials.csv')\n",
    "    \n",
    "    materials['components_total'] = (materials['quantity_1'].fillna(0) + \n",
    "                                     materials['quantity_2'].fillna(0) + \n",
    "                                     materials['quantity_3'].fillna(0) + \n",
    "                                     materials['quantity_4'].fillna(0) + \n",
    "                                     materials['quantity_5'].fillna(0) + \n",
    "                                     materials['quantity_6'].fillna(0) + \n",
    "                                     materials['quantity_7'].fillna(0) + \n",
    "                                     materials['quantity_8'].fillna(0))\n",
    "\n",
    "    materials['components_distinct'] = (materials['component_id_1'].notnull().astype(int) + \n",
    "                                        materials['component_id_2'].notnull().astype(int) + \n",
    "                                        materials['component_id_3'].notnull().astype(int) + \n",
    "                                        materials['component_id_4'].notnull().astype(int) + \n",
    "                                        materials['component_id_5'].notnull().astype(int) + \n",
    "                                        materials['component_id_6'].notnull().astype(int) + \n",
    "                                        materials['component_id_7'].notnull().astype(int) + \n",
    "                                        materials['component_id_8'].notnull().astype(int))\n",
    "    \n",
    "    # Merge selected features from bill_of_materials\n",
    "    # Just use the first component_id, ignore the others for now!\n",
    "    features = ['tube_assembly_id', 'component_id_1', 'components_total', 'components_distinct']\n",
    "    X = X.merge(materials[features], how='left')\n",
    "    \n",
    "    # Get component_type_id (has lower cardinality than component_id)\n",
    "    components = pd.read_csv(SOURCE + 'components.csv')\n",
    "    components = components.rename(columns={'component_id': 'component_id_1'})\n",
    "    features = ['component_id_1', 'component_type_id']\n",
    "    X = X.merge(components[features], how='left')\n",
    "    \n",
    "    # Count the number of specs for the tube assembly\n",
    "    specs = pd.read_csv(SOURCE + 'specs.csv')\n",
    "    specs['specs_total'] = specs.drop(columns=['tube_assembly_id']).count(axis=1)\n",
    "    features = ['tube_assembly_id', 'specs_total', 'spec1']\n",
    "    X = X.merge(specs[features], how='left')\n",
    "    \n",
    "    # Drop tube_assembly_id because our goal is to predict unknown assemblies\n",
    "    X = X.drop(columns='tube_assembly_id')\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "# Read data\n",
    "trainval = pd.read_csv(SOURCE + 'train_set.csv')\n",
    "test = pd.read_csv(SOURCE + 'test_set.csv')\n",
    "\n",
    "# Split into train & validation sets\n",
    "# All rows for a given tube_assembly_id should go in either train or validation\n",
    "trainval_tube_assemblies = trainval['tube_assembly_id'].unique()\n",
    "train_tube_assemblies, val_tube_assemblies = train_test_split(\n",
    "    trainval_tube_assemblies, random_state=42)\n",
    "train = trainval[trainval.tube_assembly_id.isin(train_tube_assemblies)]\n",
    "val = trainval[trainval.tube_assembly_id.isin(val_tube_assemblies)]\n",
    "\n",
    "# Wrangle train, validation, and test sets\n",
    "train = wrangle(train)\n",
    "val = wrangle(val)\n",
    "test = wrangle(test)\n",
    "\n",
    "# Arrange X matrix and y vector (log-transformed)\n",
    "target = 'cost'\n",
    "X_train = train.drop(columns=target)\n",
    "X_val = val.drop(columns=target)\n",
    "X_test = test.drop(columns='id')\n",
    "y_train = train[target]\n",
    "y_val = val[target]\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_val_log = np.log1p(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vit53URnH6u4"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Make pipeline\n",
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    ")\n",
    "\n",
    "# Fit\n",
    "pipeline.fit(X_train, y_train_log)\n",
    "\n",
    "# Validate\n",
    "y_pred_log = pipeline.predict(X_val)\n",
    "print('Validation Error', rmse(y_val_log, y_pred_log))\n",
    "\n",
    "# Predict\n",
    "def generate_submission(estimator, X_test, filename):\n",
    "    y_pred_log = estimator.predict(X_test)\n",
    "    y_pred = np.expm1(y_pred_log)  # Convert from log-dollars to dollars\n",
    "    submission = pd.read_csv(SOURCE + '../sample_submission.csv')\n",
    "    submission['cost'] = y_pred\n",
    "    submission.to_csv(filename, index=False)\n",
    "    \n",
    "generate_submission(pipeline, X_test, 'submission-02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "id": "OPnPE-xTRljP",
    "outputId": "6280acf5-8280-4d0a-eb79-5d72533daffa"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "rf = pipeline.named_steps['randomforestregressor']\n",
    "importances = pd.Series(rf.feature_importances_, X_train.columns)\n",
    "importances.sort_values().plot.barh(color='grey');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kKZh950URljR"
   },
   "source": [
    "## Use xgboost for gradient boosting\n",
    "\n",
    "#### [XGBoost Python API Reference: Scikit-Learn API](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ruupQ-TWjK6D",
    "outputId": "c0052958-535d-44d6-dc0e-28b954a61efd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QnpC0mHzRljS"
   },
   "source": [
    "#### Jason Brownlee, [Avoid Overfitting By Early Stopping With XGBoost In Python](https://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qb_R5_8eRljT",
    "outputId": "40e3dda5-aa1b-4985-9de1-96fda9362f57"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uu50KGLSqDK0"
   },
   "source": [
    "#### Kaggle RMSLE: 0.29454"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OCKIuAU2RljU"
   },
   "source": [
    "### Understand the difference between boosting & bagging\n",
    "\n",
    "Boosting (used by Gradient Boosting) is different than Bagging (used by Random Forests). \n",
    "\n",
    "[_An Introduction to Statistical Learning_](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf) Chapter 8.2.3, Boosting:\n",
    "\n",
    ">Recall that bagging involves creating multiple copies of the original training data set using the bootstrap, fitting a separate decision tree to each copy, and then combining all of the trees in order to create a single predictive model.\n",
    "\n",
    ">**Boosting works in a similar way, except that the trees are grown _sequentially_: each tree is grown using information from previously grown trees.**\n",
    "\n",
    ">Unlike fitting a single large decision tree to the data, which amounts to _fitting the data hard_ and potentially overfitting, the boosting approach instead _learns slowly._ Given the current model, we fit a decision tree to the residuals from the model.\n",
    "\n",
    ">We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes. **By fitting small trees to the residuals, we slowly improve fË† in areas where it does not perform well.**\n",
    "\n",
    ">Note that in boosting, unlike in bagging, the construction of each tree depends strongly on the trees that have already been grown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kXCr2NY5RljV"
   },
   "source": [
    "# Assignment\n",
    "- Continue to participate in the [Kaggle Caterpillar competition](https://www.kaggle.com/c/caterpillar-tube-pricing).\n",
    "- Do more feature engineering. \n",
    "- Use xgboost for gradient boosting.\n",
    "- Submit new predictions.\n",
    "- Commit your notebook to your fork of the GitHub repo.\n",
    "\n",
    "## Stretch Goals\n",
    "- Improve your scores on Kaggle.\n",
    "- Make visualizations and share on Slack.\n",
    "- Look at [Kaggle Kernels](https://www.kaggle.com/c/caterpillar-tube-pricing/kernels) for ideas about feature engineerng and visualization.\n",
    "- Look at the bonus notebook in the repo, about Monotonic Constraints with Gradient Boosting.\n",
    "- Read more about gradient boosting:\n",
    "  - [A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)\n",
    "  - [A Kaggle Master Explains Gradient Boosting](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/)\n",
    "  - [_An Introduction to Statistical Learning_](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf) Chapter 8\n",
    "  - [Gradient Boosting Explained](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html)\n",
    "  - [Boosting](https://www.youtube.com/watch?v=GM3CDQfQ4sw) (3 minute video)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "REFERENCE_DS5_gradient_boosting_more_feature_engineering.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
