{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-hTictxWYih7"
   },
   "source": [
    "_Lambda School Data Science_ \n",
    "\n",
    "This sprint, your project is Caterpillar Tube Pricing: Predict the prices suppliers will quote for industrial tube assemblies.\n",
    "\n",
    "# Permutation Importances, Partial Dependence Plots\n",
    "\n",
    "\n",
    "#### Objectives\n",
    "- Get and interpret permutation importances\n",
    "- Visualize and interpret partial dependence plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LoxNYFBXYih9"
   },
   "source": [
    "### Links\n",
    "- [Kaggle / Dan Becker: Machine Learning Explainability](https://www.kaggle.com/learn/machine-learning-explainability)\n",
    "  - [Permutation Importance](https://www.kaggle.com/dansbecker/permutation-importance)\n",
    "  - [Partial Dependence Plots](https://www.kaggle.com/dansbecker/partial-plots)\n",
    "- [Christoph Molnar: Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/)\n",
    "  - [(Permutation) Importance](https://christophm.github.io/interpretable-ml-book/feature-importance.html)\n",
    "  - [Partial Dependence Plots](https://christophm.github.io/interpretable-ml-book/pdp.html) + [animated explanation](https://twitter.com/ChristophMolnar/status/1066398522608635904)\n",
    "- Random Forest Feature Importances\n",
    "  - [Ando Saabas: Selecting good features, Part 3, Random Forests](https://blog.datadive.net/selecting-good-features-part-iii-random-forests/)\n",
    "  - [Terence Parr, et al: Beware Default Random Forest Importances](https://explained.ai/rf-importance/index.html)\n",
    "\n",
    "### Libraries\n",
    "- [eli5](https://github.com/TeamHG-Memex/eli5): `conda install -c conda-forge eli5` / `pip install eli5`\n",
    "- [PDPbox](https://github.com/SauceCat/PDPbox): `pip install pdpbox`\n",
    "- [category_encoders](https://github.com/scikit-learn-contrib/categorical-encoding): `conda install -c conda-forge category_encoders` / `pip install category_encoders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "BFQMky3CYih-",
    "outputId": "eb4497b7-4354-4a2d-b86e-1a8e41e6ee3a"
   },
   "outputs": [],
   "source": [
    "# !pip install eli5 pdpbox category_encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mDthquUBYiiB"
   },
   "source": [
    "### Library quirks to work around\n",
    "\n",
    "1. Some of these libraries don't work with pipelines.\n",
    "\n",
    "2. eli5 PermutationImportance + xgboost + pandas didn't work. The bug seems to be fixed now, but if you have problems, [there's a work-around:](https://www.kaggle.com/dansbecker/permutation-importance#392299)\n",
    "\n",
    "> Important note here for anyone trying to use eli5's PermutationImportance on XGBoost estimators, currently you need to train your models using \".values or .as_matrix()\" with you input data (X and Y), otherwise PermutationImportance won't work, [source](https://github.com/TeamHG-Memex/eli5/issues/256).\n",
    "\n",
    "3. PDPbox _only_ works with pandas.\n",
    "\n",
    "4. With PDPBox version <= 0.20, using the `pdp_interact_plot` function, `plot_type='contour'` gets an error, but `plot_type='grid'` works. [This issue](https://github.com/SauceCat/PDPbox/issues/40) will be fixed in the next release of PDPbox.\n",
    "\n",
    "**[(Data science is often about putting square pegs in round holes!)](https://www.youtube.com/watch?v=ry55--J4_VQ)**\n",
    "\n",
    "### Two types of model explanations today:\n",
    "\n",
    "#### 1. Global model explanation:Â all features in relation to each other\n",
    "- Feature Importances: _Default, fastest, good for first estimates_\n",
    "- Drop-Column Importances: _The best in theory, but much too slow in practice_\n",
    "- Permutaton Importances: _A good compromise!_\n",
    "\n",
    "#### 2. Global model explanation:Â individual feature(s) in relation to target\n",
    "- Partial Dependence plots\n",
    "\n",
    "### Third type of model explanation next week:\n",
    "\n",
    "#### 3. Individual prediction explanation\n",
    "- Shapley Values\n",
    "\n",
    "_Note that the coefficients from a linear model give you all three types of explanations!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0lF0mw_yYiiC"
   },
   "source": [
    "### Get data\n",
    "\n",
    "\n",
    "#### Option 1. Kaggle web UI\n",
    " \n",
    "Sign in to Kaggle and go to the [Caterpillar Tube Pricing](https://www.kaggle.com/c/caterpillar-tube-pricing) competition. Go to the Data page. After you have accepted the rules of the competition, use the download buttons to download the data.\n",
    "\n",
    "\n",
    "#### Option 2. Kaggle API\n",
    "\n",
    "Follow these [instructions](https://github.com/Kaggle/kaggle-api).\n",
    "\n",
    "#### Option 3. GitHub Repo â€” LOCAL\n",
    "\n",
    "If you are working locally:\n",
    "\n",
    "1. Clone the [GitHub repo](https://github.com/LambdaSchool/DS-Unit-2-Applied-Modeling/tree/master/data/caterpillar) locally. The data is in the repo, so you don't need to download it separately.\n",
    "\n",
    "2. Unzip the file `caterpillar-tube-pricing.zip` which is in the data folder of your local repo.\n",
    "\n",
    "3. Unzip the file `data.zip`. \n",
    "\n",
    "4. Run the cell below to assign a constant named `SOURCE`, a string that points to the location of the data on your local machine. The rest of the code in the notebook will use this constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_xw6zxTlYiiC"
   },
   "outputs": [],
   "source": [
    "SOURCE = '../data/caterpillar/caterpillar-tube-pricing/competition_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 4. GitHub Repo â€” COLAB\n",
    "\n",
    "If you are working on Google Colab, uncomment and run these cells, to download the data, unzip it, and assign a constant that points to the location of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Applied-Modeling/master/data/caterpillar/caterpillar-tube-pricing.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip caterpillar-tube-pricing.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE = 'competition_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zuzgmaBfYiiI"
   },
   "source": [
    "# Example ðŸšœ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9kMjLbFtYiiI"
   },
   "source": [
    "***We considered some questions about this relational data...***\n",
    "\n",
    "### `bill_of_materials`\n",
    "\n",
    "is formatted like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "aKwLXivaYiiJ",
    "outputId": "323c3b72-13fa-4793-e2a2-60ef6341fb3b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "materials = pd.read_csv(SOURCE + 'bill_of_materials.csv')\n",
    "materials.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zXcuBcU8YiiK"
   },
   "source": [
    "#### Would this be a better representation?\n",
    "\n",
    "Could pandas melt, crosstab, and other functions help reshape the data like this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJYsGhWHYiiL"
   },
   "source": [
    "| Crosstab | C-1622 | C-1629 | C-1312 | C-1624 | C-1631 | C-1641 | Distinct | Total |\n",
    "|:--------:|:------:|--------|--------|--------|--------|--------|----------|-------|\n",
    "| TA-00001 | 2      | 2      | 0      | 0      | 0      | 0      | 2        | 4     |\n",
    "| TA-00002 | 0      | 0      | 2      | 0      | 0      | 0      | 1        | 2     |\n",
    "| TA-00003 | 0      | 0      | 2      | 0      | 0      | 0      | 1        | 2     |\n",
    "| TA-00004 | 0      | 0      | 2      | 0      | 0      | 0      | 1        | 2     |\n",
    "| TA-00005 | 0      | 0      | 0      | 1      | 1      | 1      | 3        | 3     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mj9DvANeYiiL"
   },
   "source": [
    "### `components`\n",
    "\n",
    "Contains three representations of each component, in order of decreasing cardinality / granularity:\n",
    "\n",
    "- `component_id`\n",
    "- `name`\n",
    "- `component_type_id`\n",
    "\n",
    "What are the pros & cons of these different representations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "b64SKvg2YiiM",
    "outputId": "ba2c354e-ebc1-4b4c-835b-8f1f3ab1c37f"
   },
   "outputs": [],
   "source": [
    "components = pd.read_csv(SOURCE + 'components.csv')\n",
    "components.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pSmjDeecYiiO"
   },
   "source": [
    "***Here's how we could do some of this data wrangling...***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OgVDM0OeYiiP"
   },
   "source": [
    "### 1a. Get a tidy list of the component id's in each tube assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "ZF8YqBtBYiiQ",
    "outputId": "a55d68dc-beac-4d3a-bd0e-cc7b9a1fc4c4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "materials = pd.read_csv(SOURCE + 'bill_of_materials.csv')\n",
    "\n",
    "assembly_components = materials.melt(id_vars='tube_assembly_id', \n",
    "                                     value_vars=[f'component_id_{n}' for n in range(1,9)])\n",
    "\n",
    "assembly_components = (assembly_components\n",
    "                       .sort_values(by='tube_assembly_id')\n",
    "                       .dropna()\n",
    "                       .rename(columns={'value': 'component_id'}))\n",
    "\n",
    "assembly_components.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sl0PLnTsYiiS"
   },
   "source": [
    "### 1b. Merge with component types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "RlPI75QtYiiT",
    "outputId": "c0fc8526-16fc-409a-a165-79c6bc9b6950"
   },
   "outputs": [],
   "source": [
    "components = pd.read_csv(SOURCE + 'components.csv')\n",
    "assembly_component_types = assembly_components.merge(components, how='left')\n",
    "assembly_component_types.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NJRtPHkeYiiV"
   },
   "source": [
    "### 1c. Make a crosstab of the component types for each assembly (one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "FreT2NuUYiiV",
    "outputId": "4d9892a3-78e6-4778-de23-e0319b440706"
   },
   "outputs": [],
   "source": [
    "table = pd.crosstab(assembly_component_types['tube_assembly_id'], \n",
    "                    assembly_component_types['component_type_id'])\n",
    "\n",
    "table = table.reset_index()\n",
    "table.columns.name = ''\n",
    "print(table.shape)\n",
    "table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uwoM0NnpYiiW"
   },
   "source": [
    "### 2a. Most of the component files have a \"weight\" feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "colab_type": "code",
    "id": "90lIJ4HoYiiX",
    "outputId": "26d209be-47cb-4dc3-8ff7-e9845d2424b3"
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "def search_column(name):\n",
    "    for path in glob(SOURCE + '*.csv'):\n",
    "        df = pd.read_csv(path)\n",
    "        if name in df.columns:\n",
    "            print(path, df.shape)\n",
    "            print(df.columns.tolist(), '\\n')\n",
    "\n",
    "search_column('weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YxHns29uYiiY"
   },
   "source": [
    "### 2b. Most of the component files have \"orientation\" & \"unique_feature\" binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "mhzYhf-qYiiZ",
    "outputId": "2defc0e0-c144-4fed-da79-dff6dd3c6461"
   },
   "outputs": [],
   "source": [
    "comp_threaded = pd.read_csv(SOURCE + 'comp_threaded.csv')\n",
    "comp_threaded['orientation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "vHKqnEhuYiia",
    "outputId": "df8e6ebe-4c30-4a2a-a269-1eafac603fc3"
   },
   "outputs": [],
   "source": [
    "comp_threaded['unique_feature'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eeO9v37rYiib"
   },
   "source": [
    "### 2c. Read all the component files and concatenate them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nbxr2-7qYiic"
   },
   "outputs": [],
   "source": [
    "comp = pd.concat((pd.read_csv(path) for path in glob(SOURCE + 'comp_*.csv')), sort=False)\n",
    "columns = ['component_id', 'component_type_id', 'orientation', 'unique_feature', 'weight']\n",
    "comp = comp[columns]\n",
    "comp['orientation'] = (comp['orientation']=='Yes').astype(int)\n",
    "comp['unique_feature'] = (comp['unique_feature']=='Yes').astype(int)\n",
    "comp['weight'] = comp['weight'].fillna(comp['weight'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "ZZP-2fXIhB5X",
    "outputId": "71c53983-e614-4b63-8e1c-43bfa7ade250"
   },
   "outputs": [],
   "source": [
    "comp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UKvxK-39Yiid"
   },
   "source": [
    "### 2d. Engineer features, aggregated for all components in a tube assembly\n",
    "- Components total\n",
    "- Components distinct\n",
    "- Orientation \n",
    "- Unique Feature\n",
    "- Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "OM6mkggPYiie",
    "outputId": "cf4f161d-13dc-4366-9598-2ccd7143a3c5"
   },
   "outputs": [],
   "source": [
    "materials['components_total'] = sum(materials[f'quantity_{n}'].fillna(0)  for n in range(1,9))\n",
    "materials['components_distinct'] = sum(materials[f'component_id_{n}'].notnull().astype(int) for n in range(1,9))\n",
    "materials['orientation'] = 0\n",
    "materials['unique_feature'] = 0\n",
    "materials['weight'] = 0\n",
    "\n",
    "for n in range(1,9):\n",
    "    materials = materials.merge(comp, left_on=f'component_id_{n}', right_on='component_id', \n",
    "                                how='left', suffixes=('', f'_{n}'))\n",
    "\n",
    "for col in materials:\n",
    "    if 'orientation' in col or 'unique_feature' in col or 'weight' in col:\n",
    "        materials[col] = materials[col].fillna(0)\n",
    "        \n",
    "materials['orientation'] = sum(materials[f'orientation_{n}'] for n in range(1,9))\n",
    "materials['unique_feature'] = sum(materials[f'unique_feature_{n}'] for n in range(1,9))\n",
    "materials['weight'] = sum(materials[f'weight_{n}'] for n in range(1,9))\n",
    "\n",
    "materials.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "aRG6ALKLhhYz",
    "outputId": "69fb2098-17a8-4b6d-d5db-25cf6cbc01fb"
   },
   "outputs": [],
   "source": [
    "features = ['tube_assembly_id', 'orientation', 'unique_feature', 'weight', \n",
    "            'components_total', 'components_distinct', 'component_id_1']\n",
    "materials = materials[features]\n",
    "print(materials.shape)\n",
    "materials.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xo4IsiIUYiif"
   },
   "source": [
    "### 3. Read tube data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "iY8ixDQJYiig",
    "outputId": "6ea66cb5-24d3-4dfd-ba09-b89cd95e2c94"
   },
   "outputs": [],
   "source": [
    "tube = pd.read_csv(SOURCE + 'tube.csv')\n",
    "tube.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OepDqvsRYiii"
   },
   "source": [
    "### 4. Merge all this data with train, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FCPcx32kYiii"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read data\n",
    "trainval = pd.read_csv(SOURCE + 'train_set.csv')\n",
    "test = pd.read_csv(SOURCE + 'test_set.csv')\n",
    "\n",
    "# Split into train & validation sets\n",
    "# All rows for a given tube_assembly_id should go in either train or validation\n",
    "trainval_tube_assemblies = trainval['tube_assembly_id'].unique()\n",
    "train_tube_assemblies, val_tube_assemblies = train_test_split(\n",
    "    trainval_tube_assemblies, random_state=42)\n",
    "train = trainval[trainval.tube_assembly_id.isin(train_tube_assemblies)]\n",
    "val = trainval[trainval.tube_assembly_id.isin(val_tube_assemblies)]\n",
    "\n",
    "# Wrangle train, validation, and test sets\n",
    "def wrangle(X):\n",
    "    X = X.copy()\n",
    "    \n",
    "    # Engineer date features\n",
    "    X['quote_date'] = pd.to_datetime(X['quote_date'], infer_datetime_format=True)\n",
    "    X['quote_date_year'] = X['quote_date'].dt.year\n",
    "    X['quote_date_month'] = X['quote_date'].dt.month\n",
    "    X = X.drop(columns='quote_date')\n",
    "    \n",
    "    # Merge data\n",
    "    X = (X.merge(table, how='left')\n",
    "         .merge(materials, how='left')\n",
    "         .merge(tube, how='left')\n",
    "         .fillna(0))\n",
    "    \n",
    "    # Drop tube_assembly_id because our goal is to predict unknown assemblies\n",
    "    X = X.drop(columns='tube_assembly_id')\n",
    "    return X\n",
    "\n",
    "train = wrangle(train)\n",
    "val = wrangle(val)\n",
    "test = wrangle(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FUjdGlFcYiik"
   },
   "source": [
    "### 5. Arrange X matrix and y vector (log-transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3uHk6IOzYiil"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "target = 'cost'\n",
    "X_train = train.drop(columns=target)\n",
    "X_val = val.drop(columns=target)\n",
    "X_test = test.drop(columns='id')\n",
    "y_train = train[target]\n",
    "y_val = val[target]\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_val_log = np.log1p(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d2qhAjycYiin"
   },
   "source": [
    "### 6. Use xgboost to fit and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 29903
    },
    "colab_type": "code",
    "id": "syxuHpu2Yiin",
    "outputId": "75e455d1-d263-451d-b873-7d4501a69ed7"
   },
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "encoder = ce.OrdinalEncoder()\n",
    "X_train_encoded = encoder.fit_transform(X_train)\n",
    "X_val_encoded = encoder.transform(X_val)\n",
    "eval_set = [(X_train_encoded, y_train_log), \n",
    "            (X_val_encoded, y_val_log)]\n",
    "model = XGBRegressor(n_estimators=2000, n_jobs=-1)\n",
    "model.fit(X_train_encoded, y_train_log, \n",
    "          eval_set=eval_set, eval_metric='rmse', early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "colab_type": "code",
    "id": "Yi0j5IgvYiip",
    "outputId": "60f402e9-aa50-4949-837b-bfa3170fe08f"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = model.evals_result()\n",
    "train_rmse = results['validation_0']['rmse']\n",
    "val_rmse = results['validation_1']['rmse']\n",
    "epoch = range(len(train_rmse))\n",
    "plt.plot(epoch, train_rmse, label='Train')\n",
    "plt.plot(epoch, val_rmse, label='Validation')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kops860zYiis"
   },
   "source": [
    "### 7. Generate submission for Kaggle\n",
    "\n",
    "Scores for this submission:\n",
    "\n",
    "- Public: 0.26083\n",
    "- Private: 0.28639"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sn77EPtAYiis"
   },
   "outputs": [],
   "source": [
    "def generate_submission(estimator, X_test, filename):\n",
    "    y_pred_log = estimator.predict(X_test)\n",
    "    y_pred = np.expm1(y_pred_log)  # Convert from log-dollars to dollars\n",
    "    submission = pd.read_csv(SOURCE + '../sample_submission.csv')\n",
    "    submission['cost'] = y_pred\n",
    "    submission.to_csv(filename, index=False)\n",
    "    \n",
    "X_test_encoded = encoder.transform(X_test)\n",
    "generate_submission(model, X_test_encoded, 'submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7HOayKBOYiit"
   },
   "source": [
    "# MODEL INTERPRETATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4bRhsxENYiiu"
   },
   "source": [
    "## 1a. Feature Importances\n",
    "- Global explanation:Â all features in relation to each other\n",
    "- Default, fastest, good for first estimates\n",
    "\n",
    "[Here's some food for thought](https://blog.datadive.net/selecting-good-features-part-iii-random-forests/) about feature importances:\n",
    "\n",
    ">**When the dataset has two (or more) correlated features, then from the point of view of the model, any of these correlated features can be used as the predictor, with no concrete preference of one over the others.** But once one of them is used, the importance of others is significantly reduced since effectively the impurity they can remove is already removed by the first feature. As a consequence, they will have a lower reported importance. This is not an issue when we want to use feature selection to reduce overfitting, since it makes sense to remove features that are mostly duplicated by other features. But when interpreting the data, it can lead to the incorrect conclusion that one of the variables is a strong predictor while the others in the same group are unimportant, while actually they are very close in terms of their relationship with the response variable.\n",
    "\n",
    "For more information, see [Beware Default Random Forest Importances](https://explained.ai/rf-importance/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1613
    },
    "colab_type": "code",
    "id": "BNVm6f7mYiiu",
    "outputId": "a5097e9d-bda7-4ce9-91ed-e58e0eaeb992"
   },
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = pd.Series(model.feature_importances_, X_train_encoded.columns)\n",
    "\n",
    "# Plot feature importances\n",
    "n = len(X_train_encoded.columns)\n",
    "plt.figure(figsize=(10,n/2))\n",
    "plt.title(f'Top {n} features')\n",
    "importances.sort_values()[-n:].plot.barh(color='grey');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y8HzLcCBYiiv"
   },
   "source": [
    "## 1b. Drop-Column Importance\n",
    "- Global explanation:Â all features in relation to each other\n",
    "- The best in theory, but much too slow in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "DQAOlERnYiiw",
    "outputId": "9426ed52-562d-45ef-d14b-d38dc85452d0"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "column  = 'annual_usage'\n",
    "\n",
    "# Fit without column\n",
    "model = XGBRegressor(n_estimators=1000, n_jobs=-1)\n",
    "model.fit(X_train_encoded.drop(columns=column), y_train_log)\n",
    "y_pred_log = model.predict(X_val_encoded.drop(columns=column))\n",
    "score_without = rmse(y_val_log, y_pred_log)\n",
    "print(f'Validation RMSLE without {column}:', score_without)\n",
    "\n",
    "# Fit with column\n",
    "model = XGBRegressor(n_estimators=1000, n_jobs=-1)\n",
    "model.fit(X_train_encoded, y_train_log)\n",
    "y_pred_log = model.predict(X_val_encoded)\n",
    "score_with = rmse(y_val_log, y_pred_log)\n",
    "print(f'Validation RMSLE with {column}:', score_with)\n",
    "\n",
    "# Compare the error with & without column\n",
    "print(f'Drop-Column Importance for {column}:', score_without - score_with)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Vu39wGkYiix"
   },
   "source": [
    "## 1c. Permutation Importance\n",
    "- Global explanation:Â all features in relation to each other\n",
    "- A good compromise!\n",
    "\n",
    "Permutation Importance is a compromise between Feature Importance based on impurity reduction (which is the fastest) and Drop Column Importance (which is the \"best.\")\n",
    "\n",
    "[The ELI5 library documentation explains,](https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html)\n",
    "\n",
    "> Importance can be measured by looking at how much the score (accuracy, F1, R^2, etc. - any score weâ€™re interested in) decreases when a feature is not available.\n",
    ">\n",
    "> To do that one can remove feature from the dataset, re-train the estimator and check the score. But it requires re-training an estimator for each feature, which can be computationally intensive. ...\n",
    ">\n",
    ">To avoid re-training the estimator we can remove a feature only from the test part of the dataset, and compute score without using this feature. It doesnâ€™t work as-is, because estimators expect feature to be present. So instead of removing a feature we can replace it with random noise - feature column is still there, but it no longer contains useful information. This method works if noise is drawn from the same distribution as original feature values (as otherwise estimator may fail). The simplest way to get such noise is to shuffle values for a feature, i.e. use other examplesâ€™ feature values - this is how permutation importance is computed.\n",
    ">\n",
    ">The method is most suitable for computing feature importances when a number of columns (features) is not huge; it can be resource-intensive otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GYCiEx7zYiiy"
   },
   "source": [
    "### Do-It-Yourself way, for intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "TksOf_n2Yiiy",
    "outputId": "2a694213-38bf-4fde-da25-dc20f636b481"
   },
   "outputs": [],
   "source": [
    "feature = 'quantity'\n",
    "X_val_encoded[feature].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "-plycNueYii0",
    "outputId": "01cee519-3c3b-4573-c097-fa8cc7df80f8"
   },
   "outputs": [],
   "source": [
    "X_val_encoded[feature].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "OF32BQb1Yii1",
    "outputId": "973b9268-3dd1-4ac8-efda-806ba7c9505c"
   },
   "outputs": [],
   "source": [
    "X_val_permuted = X_val_encoded.copy()\n",
    "X_val_permuted[feature] = np.random.permutation(X_val_encoded[feature])\n",
    "X_val_permuted[feature].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "HuyRGVpGYii2",
    "outputId": "ac174a5d-42d4-4cb3-dfe0-2888e4c11276"
   },
   "outputs": [],
   "source": [
    "X_val_permuted['quantity'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Zj8rAwMhYii4",
    "outputId": "63bed842-2fc2-466b-ae12-8f20bfddca62"
   },
   "outputs": [],
   "source": [
    "y_pred_log = model.predict(X_val_permuted)\n",
    "score_permuted = rmse(y_val_log, y_pred_log)\n",
    "print(f'Validation RMSLE with {feature}:', score_with)\n",
    "print(f'Validation RMSLE with {feature} permuted:', score_permuted)\n",
    "print(f'Permutation Importance:', score_permuted - score_with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "m5ID-5jtYii5",
    "outputId": "69ea6a5e-122b-4640-d258-62f92d207dbe"
   },
   "outputs": [],
   "source": [
    "feature = 'annual_usage'\n",
    "X_val_permuted = X_val_encoded.copy()\n",
    "X_val_permuted[feature] = np.random.permutation(X_val_encoded[feature])\n",
    "y_pred_log = model.predict(X_val_permuted)\n",
    "score_permuted = rmse(y_val_log, y_pred_log)\n",
    "print(f'Validation RMSLE with {feature}:', score_with)\n",
    "print(f'Validation RMSLE with {feature} permuted:', score_permuted)\n",
    "print(f'Permutation Importance:', score_permuted - score_with)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0LYk19SNYii7"
   },
   "source": [
    "### With eli5 library\n",
    "\n",
    "For more documentation on using this library, see:\n",
    "- [eli5.sklearn.PermutationImportance](https://eli5.readthedocs.io/en/latest/autodocs/sklearn.html#eli5.sklearn.permutation_importance.PermutationImportance)\n",
    "- [eli5.show_weights](https://eli5.readthedocs.io/en/latest/autodocs/eli5.html#eli5.show_weights)\n",
    "- [scikit-learn user guide, `scoring` parameter](https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1003
    },
    "colab_type": "code",
    "id": "zdoMW4sCYii7",
    "outputId": "31362a42-552b-486b-b7b5-4c3bba47e8e3"
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "permuter = PermutationImportance(model, scoring='neg_mean_squared_error', \n",
    "                                 cv='prefit', n_iter=2, random_state=42)\n",
    "\n",
    "permuter.fit(X_val_encoded, y_val_log)\n",
    "feature_names = X_val_encoded.columns.tolist()\n",
    "eli5.show_weights(permuter, top=None, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q07yW9k-Yii8"
   },
   "source": [
    "### We can use importances for feature selection\n",
    "\n",
    "For example, we can remove features with zero importance. The model trains faster and the score does not decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tZrPFyEMYii9",
    "outputId": "3d1a4c85-6bab-4cab-cbca-88ea95520dc1"
   },
   "outputs": [],
   "source": [
    "print('Shape before removing features:', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "A_NbDgh3Yii-",
    "outputId": "c4f7432f-d803-4e4a-8459-ec7b7c820703"
   },
   "outputs": [],
   "source": [
    "mask = permuter.feature_importances_ > 0\n",
    "features = X_train.columns[mask]\n",
    "X_train = X_train[features]\n",
    "print('Shape after removing features:', X_train.shape)\n",
    "X_val = X_val[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Erg_PdpoYii_",
    "outputId": "a38a1bbf-2718-4e7e-de5d-44e4e2629bdc"
   },
   "outputs": [],
   "source": [
    "encoder = ce.OrdinalEncoder()\n",
    "X_train_encoded = encoder.fit_transform(X_train)\n",
    "X_val_encoded = encoder.transform(X_val)\n",
    "model = XGBRegressor(n_estimators=1000, n_jobs=-1)\n",
    "model.fit(X_train_encoded, y_train_log)\n",
    "y_pred_log = model.predict(X_val_encoded)\n",
    "print(f'Validation RMSLE', rmse(y_val_log, y_pred_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOUzbLKpYijB"
   },
   "source": [
    "## 2. Partial Dependence Plots\n",
    "\n",
    "PDPbox\n",
    "- [Gallery](https://github.com/SauceCat/PDPbox#gallery)\n",
    "- [API Reference: pdpbox.pdp.pdp_isolate](https://pdpbox.readthedocs.io/en/latest/pdp_isolate.html)\n",
    "- [API Reference: pdpbox.pdp.pdp_plot](https://pdpbox.readthedocs.io/en/latest/pdp_plot.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "colab_type": "code",
    "id": "DeH3lw0CYijB",
    "outputId": "f54240ba-8525-4978-e5cd-9763d17d93fc"
   },
   "outputs": [],
   "source": [
    "from pdpbox.pdp import pdp_isolate, pdp_plot\n",
    "\n",
    "feature = 'quantity'\n",
    "\n",
    "isolated = pdp_isolate(\n",
    "    model=model, \n",
    "    dataset=X_val_encoded, \n",
    "    model_features=X_val_encoded.columns, \n",
    "    feature=feature\n",
    ")\n",
    "\n",
    "pdp_plot(isolated, feature_name=feature);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "colab_type": "code",
    "id": "FWVwA6viYijD",
    "outputId": "1e564fd9-4a08-4bc1-ed05-a534a5536b9d"
   },
   "outputs": [],
   "source": [
    "feature = 'weight'\n",
    "\n",
    "isolated = pdp_isolate(\n",
    "    model=model, \n",
    "    dataset=X_val_encoded, \n",
    "    model_features=X_val_encoded.columns, \n",
    "    feature=feature\n",
    ")\n",
    "\n",
    "pdp_plot(isolated, feature_name=feature);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oMYkx9fAYijF"
   },
   "source": [
    "### Explaining Partial Dependence Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5O6s9jisYijI"
   },
   "source": [
    "From [PDPbox documentation](https://pdpbox.readthedocs.io/en/latest/):\n",
    "\n",
    "\n",
    ">**The common headache**: When using black box machine learning algorithms like random forest and boosting, it is hard to understand the relations between predictors and model outcome. For example, in terms of random forest, all we get is the feature importance. Although we can know which feature is significantly influencing the outcome based on the importance calculation, it really sucks that we donâ€™t know in which direction it is influencing. And in most of the real cases, the effect is non-monotonic. We need some powerful tools to help understanding the complex relations between predictors and model prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zN2C8QTMYijI"
   },
   "source": [
    "[Animation by Christoph Molnar](https://twitter.com/ChristophMolnar/status/1066398522608635904), author of [_Interpretable Machine Learning_](https://christophm.github.io/interpretable-ml-book/)\n",
    "\n",
    "> Partial dependence plots show how a feature affects predictions of a Machine Learning model on average.\n",
    "> 1. Define grid along feature\n",
    "> 2. Model predictions at grid points\n",
    "> 3. Line per data instance -> ICE (Individual Conditional Expectation) curve\n",
    "> 4. Average curves to get a PDP (Partial Dependence Plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LOu_hUU6YijJ"
   },
   "source": [
    "### Partial Dependence Plots with 2 features, to see interactions\n",
    "\n",
    "PDPbox\n",
    "- [Gallery](https://github.com/SauceCat/PDPbox#gallery)\n",
    "- [API Reference: pdpbox.pdp.pdp_interact](https://pdpbox.readthedocs.io/en/latest/pdp_interact.html)\n",
    "- [API Reference: pdpbox.pdp.pdp_interact_plot](https://pdpbox.readthedocs.io/en/latest/pdp_interact_plot.html)\n",
    "\n",
    "Be aware of a bug in PDPBox version <= 0.20:\n",
    "- With the `pdp_interact_plot` function, `plot_type='contour'` gets an error, but `plot_type='grid'` works\n",
    "- This will be fixed in the next release of PDPbox: https://github.com/SauceCat/PDPbox/issues/40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "colab_type": "code",
    "id": "edL2X3QtYijJ",
    "outputId": "9372544b-7703-4b4a-d821-f40e799d8c99"
   },
   "outputs": [],
   "source": [
    "from pdpbox.pdp import pdp_interact, pdp_interact_plot\n",
    "\n",
    "features = ['quantity', 'weight']\n",
    "\n",
    "interaction = pdp_interact(\n",
    "    model=model, \n",
    "    dataset=X_val_encoded, \n",
    "    model_features=X_val_encoded.columns, \n",
    "    features=features\n",
    ")\n",
    "\n",
    "pdp_interact_plot(interaction, plot_type='grid', feature_names=features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "colab_type": "code",
    "id": "3yRW-0wIYijL",
    "outputId": "c7d13bdf-812f-4150-e33f-a93e04967789"
   },
   "outputs": [],
   "source": [
    "features = ['annual_usage', 'quote_date_year']\n",
    "\n",
    "interaction = pdp_interact(\n",
    "    model=model, \n",
    "    dataset=X_val_encoded, \n",
    "    model_features=X_val_encoded.columns, \n",
    "    features=features\n",
    ")\n",
    "\n",
    "pdp_interact_plot(interaction, plot_type='grid', feature_names=features);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zP1RIqnfYijM"
   },
   "source": [
    "# ASSIGNMENT\n",
    "- Use the Caterpillar dataset (or _any_ dataset of your choice). **Make these 3 types of visualizations** for model interpretation:\n",
    "  - Feature Importances\n",
    "  - Permutation Importances\n",
    "  - Partial Dependence Plot\n",
    "- **Share at least 1 of your visualizations on Slack.**\n",
    "- Commit your notebook to your fork of the GitHub repo.\n",
    "\n",
    "\n",
    "## Stretch Goals\n",
    "- Improve your scores on Kaggle! Look at [Kaggle Kernels](https://www.kaggle.com/c/caterpillar-tube-pricing/kernels) for ideas. **Share your best features and techniques on Slack.**\n",
    "- Try the [Skater library](https://oracle.github.io/Skater/index.html), which is an another option to get permutation importances and partial dependence plots.\n",
    "- Can you figure out partial dependence plots with categorical features?\n",
    "- Check out the links at the top of this notebook to learn more about how to interpret \"black box\" machine learning models."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "permutation_importances_partial_dependence_plots_LIVE_LESSON.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
