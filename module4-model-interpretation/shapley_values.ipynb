{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MqLTg0R5d1YQ"
   },
   "source": [
    "_Lambda School Data Science_ \n",
    "\n",
    "# Shapley Values\n",
    "\n",
    "### Objectives\n",
    "- Make decisions with probability calibration and expected value calculations\n",
    "- Explain decisions with shapley value plots\n",
    "\n",
    "### Libraries\n",
    "\n",
    "#### category_encoders\n",
    "- Local Anaconda: `conda install -c conda-forge category_encoders`\n",
    "- Google Colab: `pip install category_encoders`\n",
    "\n",
    "#### [shap](https://github.com/slundberg/shap) (for shapley value plots)\n",
    "- Local Anaconda: `conda install -c conda-forge shap` ***(I'm getting import errors locally)***\n",
    "- Google Colab: `pip install shap`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qiRSrsZQeeL_"
   },
   "outputs": [],
   "source": [
    "# !pip install category_encoders shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BvuKGqHzd1YR"
   },
   "source": [
    "## Lending Club Review ðŸ¦\n",
    "\n",
    "This notebook uses Lending Club data, historical and current. Predict if peer-to-peer loans are charged off or fully paid. Decide which loans to invest in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MM-_wd7Zd1YS"
   },
   "outputs": [],
   "source": [
    "history_location = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Classification-2/master/data/lending-club-subset.csv'\n",
    "current_location = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Classification-2/master/data/primaryMarketNotes_browseNotes_1-RETAIL.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CPMl7luDd1YT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_rows = 200\n",
    "\n",
    "# Stratified sample, 10% of expired Lending Club loans, grades A-D\n",
    "# Source: https://www.lendingclub.com/info/download-data.action\n",
    "history = pd.read_csv(history_location)\n",
    "history['issue_d'] = pd.to_datetime(history['issue_d'], infer_datetime_format=True)\n",
    "\n",
    "# Current loans available for manual investing, June 17, 2019\n",
    "# Source: https://www.lendingclub.com/browse/browse.action\n",
    "current = pd.read_csv(current_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Aten2nECd1YV"
   },
   "outputs": [],
   "source": [
    "# Transform earliest_cr_line to an integer:\n",
    "# How many days the earliest credit line was open, before the loan was issued.\n",
    "# For current loans available for manual investing, assume the loan will be issued today.\n",
    "history['earliest_cr_line'] = pd.to_datetime(history['earliest_cr_line'], infer_datetime_format=True)\n",
    "history['earliest_cr_line'] = history['issue_d'] - history['earliest_cr_line']\n",
    "history['earliest_cr_line'] = history['earliest_cr_line'].dt.days\n",
    "\n",
    "current['earliest_cr_line'] = pd.to_datetime(current['earliest_cr_line'], infer_datetime_format=True)\n",
    "current['earliest_cr_line'] = pd.Timestamp.today() - current['earliest_cr_line']\n",
    "current['earliest_cr_line'] = current['earliest_cr_line'].dt.days\n",
    "\n",
    "# Transform earliest_cr_line for the secondary applicant\n",
    "history['sec_app_earliest_cr_line'] = pd.to_datetime(history['sec_app_earliest_cr_line'], infer_datetime_format=True, errors='coerce')\n",
    "history['sec_app_earliest_cr_line'] = history['issue_d'] - history['sec_app_earliest_cr_line']\n",
    "history['sec_app_earliest_cr_line'] = history['sec_app_earliest_cr_line'].dt.days\n",
    "\n",
    "current['sec_app_earliest_cr_line'] = pd.to_datetime(current['sec_app_earliest_cr_line'], infer_datetime_format=True, errors='coerce')\n",
    "current['sec_app_earliest_cr_line'] = pd.Timestamp.today() - current['sec_app_earliest_cr_line']\n",
    "current['sec_app_earliest_cr_line'] = current['sec_app_earliest_cr_line'].dt.days\n",
    "\n",
    "# Engineer features for issue date year & month\n",
    "history['issue_d_year'] = history['issue_d'].dt.year\n",
    "history['issue_d_month'] = history['issue_d'].dt.month\n",
    "\n",
    "current['issue_d_year'] = pd.Timestamp.today().year\n",
    "current['issue_d_month'] = pd.Timestamp.today().month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RCaWx8gXd1YX"
   },
   "outputs": [],
   "source": [
    "# Calculate percent of each loan repaid\n",
    "history['percent_paid'] = history['total_pymnt'] / history['funded_amnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ROZw3r97d1YY"
   },
   "outputs": [],
   "source": [
    "# Train on the historical data.\n",
    "# For the target, use `loan_status` ('Fully Paid' or 'Charged Off')\n",
    "target = 'loan_status'\n",
    "X = history.drop(columns=target)\n",
    "y = history[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115
    },
    "colab_type": "code",
    "id": "GcjHM-5Ld1Ya",
    "outputId": "271f277f-ff85-4cc4-e888-2925ead34f69"
   },
   "outputs": [],
   "source": [
    "# Do train/validate/test 3-way split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=20000, stratify=y, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=20000, \n",
    "    stratify=y_trainval, random_state=42)\n",
    "\n",
    "print('X_train shape', X_train.shape)\n",
    "print('y_train shape', y_train.shape)\n",
    "print('X_val shape', X_val.shape)\n",
    "print('y_val shape', y_val.shape)\n",
    "print('X_test shape', X_test.shape)\n",
    "print('y_test shape', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "JAwsZEKGd1Yc",
    "outputId": "dd3dd019-78cd-4490-c259-9356c8c4d4f7"
   },
   "outputs": [],
   "source": [
    "# Save the actual results, to compare later with predicted results\n",
    "cols = ['id', 'issue_d', 'grade', 'percent_paid', 'term', 'int_rate']\n",
    "result_train = X_train[cols].copy()\n",
    "result_val = X_val[cols].copy()\n",
    "result_test = X_test[cols].copy()\n",
    "\n",
    "result_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IQiBnf1-d1Yd"
   },
   "outputs": [],
   "source": [
    "# Use Python sets to compare the historical columns & current columns\n",
    "common_columns = set(history.columns) & set(current.columns)\n",
    "just_history = set(history.columns) - set(current.columns)\n",
    "just_current = set(current.columns) - set(history.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "haq8i5azd1Yf"
   },
   "outputs": [],
   "source": [
    "# For features, use only the common columns shared by the historical & current data.\n",
    "features = list(common_columns)\n",
    "X_train = X_train[features]\n",
    "X_val = X_val[features]\n",
    "X_test = X_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "colab_type": "code",
    "id": "GW2lh-Zrd1Yi",
    "outputId": "b66467bd-3bc6-480e-df90-3faf682b45be"
   },
   "outputs": [],
   "source": [
    "def wrangle(X):\n",
    "    X = X.copy()\n",
    "\n",
    "    # Engineer new feature for every feature: is the feature null?\n",
    "    for col in X:\n",
    "        X[col+'_NULL'] = X[col].isnull()\n",
    "    \n",
    "    # Convert percentages from strings to floats\n",
    "    X['int_rate'] = X['int_rate'].str.strip('%').astype(float)\n",
    "    X['revol_util'] = X['revol_util'].str.strip('%').astype(float)\n",
    "    \n",
    "    # Convert employment length from string to float\n",
    "    X['emp_length'] = X['emp_length'].str.replace(r'\\D','').astype(float)\n",
    "        \n",
    "    # Create features for three employee titles: teacher, manager, owner\n",
    "    X['emp_title'] = X['emp_title'].str.lower()\n",
    "    X['emp_title_teacher'] = X['emp_title'].str.contains('teacher', na=False)\n",
    "    X['emp_title_manager'] = X['emp_title'].str.contains('manager', na=False)\n",
    "    X['emp_title_owner']   = X['emp_title'].str.contains('owner', na=False)\n",
    "\n",
    "    # Get length of free text fields\n",
    "    X['title'] = X['title'].str.len()\n",
    "    X['desc'] = X['desc'].str.len()\n",
    "    X['emp_title'] = X['emp_title'].str.len()\n",
    "    \n",
    "    # Convert sub_grade from string \"A1\"-\"D5\" to integer 1-20\n",
    "    sub_grade_ranks = {'A1': 1, 'A2': 2, 'A3': 3, 'A4': 4, 'A5': 5, 'B1': 6, 'B2': 7, \n",
    "                       'B3': 8, 'B4': 9, 'B5': 10, 'C1': 11, 'C2': 12, 'C3': 13, 'C4': 14, \n",
    "                       'C5': 15, 'D1': 16, 'D2': 17, 'D3': 18, 'D4': 19, 'D5': 20}\n",
    "    X['sub_grade'] = X['sub_grade'].map(sub_grade_ranks)\n",
    "    \n",
    "    # Drop some columns\n",
    "    X = X.drop(columns='id')        # Always unique\n",
    "    X = X.drop(columns='url')       # Always unique\n",
    "    X = X.drop(columns='member_id') # Always null\n",
    "    X = X.drop(columns='grade')     # Duplicative of sub_grade\n",
    "    X = X.drop(columns='zip_code')  # High cardinality\n",
    "    \n",
    "    # Only use these features which had nonzero permutation importances in earlier models    \n",
    "    features = ['acc_open_past_24mths', 'addr_state', 'all_util', 'annual_inc', \n",
    "                'annual_inc_joint', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util', \n",
    "                'collections_12_mths_ex_med', 'delinq_amnt', 'desc_NULL', 'dti', \n",
    "                'dti_joint', 'earliest_cr_line', 'emp_length', 'emp_length_NULL', \n",
    "                'emp_title', 'emp_title_NULL', 'emp_title_owner', 'fico_range_high', \n",
    "                'funded_amnt', 'home_ownership', 'inq_last_12m', 'inq_last_6mths', \n",
    "                'installment', 'int_rate', 'issue_d_month', 'issue_d_year', 'loan_amnt', \n",
    "                'max_bal_bc', 'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', \n",
    "                'mo_sin_rcnt_rev_tl_op', 'mort_acc', 'mths_since_last_major_derog_NULL', \n",
    "                'mths_since_last_record', 'mths_since_recent_bc', 'mths_since_recent_inq', \n",
    "                'num_actv_bc_tl', 'num_actv_rev_tl', 'num_op_rev_tl', 'num_rev_tl_bal_gt_0', \n",
    "                'num_tl_120dpd_2m_NULL', 'open_rv_12m_NULL', 'open_rv_24m', \n",
    "                'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'pub_rec_bankruptcies', 'purpose', \n",
    "                'revol_bal', 'revol_bal_joint', 'sec_app_earliest_cr_line', \n",
    "                'sec_app_fico_range_high', 'sec_app_open_acc', 'sec_app_open_act_il', \n",
    "                'sub_grade', 'term', 'title', 'title_NULL', 'tot_coll_amt', \n",
    "                'tot_hi_cred_lim', 'total_acc', 'total_bal_il', 'total_bc_limit', \n",
    "                'total_cu_tl', 'total_rev_hi_lim']    \n",
    "    X = X[features]\n",
    "    \n",
    "    # Return the wrangled dataframe\n",
    "    return X\n",
    "\n",
    "\n",
    "X_train = wrangle(X_train)\n",
    "X_val   = wrangle(X_val)\n",
    "X_test  = wrangle(X_test)\n",
    "\n",
    "print('X_train shape', X_train.shape)\n",
    "print('X_val shape', X_val.shape)\n",
    "print('X_test shape', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3771
    },
    "colab_type": "code",
    "id": "M52FAeDpd1Yk",
    "outputId": "ae62c5ad-eeca-4fc1-fc01-de14d48101b6"
   },
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "processor = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    SimpleImputer(strategy='median')\n",
    ")\n",
    "\n",
    "X_train_processed = processor.fit_transform(X_train)\n",
    "X_val_processed = processor.transform(X_val)\n",
    "\n",
    "eval_set = [(X_train_processed, y_train), \n",
    "            (X_val_processed, y_val)]\n",
    "\n",
    "model = XGBClassifier(n_estimators=1000, n_jobs=-1)\n",
    "model.fit(X_train_processed, y_train, eval_set=eval_set, eval_metric='auc', \n",
    "          early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "hpiXSK4xd1Y7",
    "outputId": "010c7432-4471-4c91-afb7-55517eb0f5fa"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "X_test_processed = processor.transform(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "print('Test ROC AUC:', roc_auc_score(y_test, y_pred_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8MLK-DNd1ZA"
   },
   "source": [
    "## Explain decisions with shapley value plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DXZFws3Jd1ZA"
   },
   "source": [
    "### We're learning about 3 types of model explanations this unit:\n",
    "\n",
    "#### Global explanation:Â all features in relation to each other\n",
    "What features have the most impact on my model's predictions?\n",
    "\n",
    "- Feature Importances: _Default, fastest, good for first estimates_\n",
    "- Drop-Column Importances: _The best in theory, but much too slow in practice_\n",
    "- Permutaton Importances: _A good compromise!_\n",
    "\n",
    "#### Global explanation:Â individual feature(s) in relation to target\n",
    "How do my model's predictions change if I vary some feature(s) and hold the other features constant?\n",
    "\n",
    "- Partial Dependence plots\n",
    "\n",
    "#### Individual prediction explanation\n",
    "Why does my model make this prediction for this individual observation? \n",
    "\n",
    "- Shapley Values\n",
    "\n",
    "_Note that the coefficients from a linear model give you all three types of explanations!_\n",
    "\n",
    "#### [Dan Becker explains Shapley Values:](https://www.kaggle.com/dansbecker/shap-values)\n",
    "\n",
    ">You've seen (and used) techniques to extract general insights from a machine learning model. But what if you want to break down how the model works for an individual prediction?\n",
    "\n",
    ">SHAP Values (an acronym from SHapley Additive exPlanations) break down a prediction to show the impact of each feature. \n",
    "\n",
    ">There is some complexity to the technique ... We won't go into that detail here, since it isn't critical for using the technique. [This blog post](https://towardsdatascience.com/one-feature-attribution-method-to-supposedly-rule-them-all-shapley-values-f3e04534983d) has a longer theoretical explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1197
    },
    "colab_type": "code",
    "id": "R2mZ_gfVd1ZA",
    "outputId": "17908c49-985a-44b2-b358-18f8918a022e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df = result_test.copy()\n",
    "df['status_group'] = y_test\n",
    "df['pred_proba'] = y_pred_proba\n",
    "threshold = np.percentile(y_pred_proba, 75)\n",
    "possible = df[y_pred_proba > threshold]\n",
    "picks = possible.sample(40, random_state=42).copy()\n",
    "picks.sort_values(by='pred_proba', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "colab_type": "code",
    "id": "H0sV1mc7d1ZC",
    "outputId": "644be59e-4fcf-4157-c03e-aeb0d8f1cf70"
   },
   "outputs": [],
   "source": [
    "data_for_prediction = X_test[X_test.index==92946]\n",
    "data_for_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w9vCoRZn37rz"
   },
   "outputs": [],
   "source": [
    "data_for_prediction_processed = processor.transform(data_for_prediction)\n",
    "data_for_prediction_processed = pd.DataFrame(data_for_prediction_processed)\n",
    "data_for_prediction_processed.columns = data_for_prediction.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192
    },
    "colab_type": "code",
    "id": "CvdSEpAVd1ZD",
    "outputId": "f7f908e4-9a41-4f37-c6fb-e8fdcc1cbf62"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(data_for_prediction_processed)\n",
    "shap.force_plot(explainer.expected_value, shap_values, data_for_prediction_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192
    },
    "colab_type": "code",
    "id": "bgUNdPux4RRX",
    "outputId": "15afdf38-14a8-44fc-8103-36fed27dda88"
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "data_for_prediction = X_test[X_test.index==80454]\n",
    "data_for_prediction\n",
    "data_for_prediction_processed = processor.transform(data_for_prediction)\n",
    "data_for_prediction_processed = pd.DataFrame(data_for_prediction_processed)\n",
    "data_for_prediction_processed.columns = data_for_prediction.columns\n",
    "shap_values = explainer.shap_values(data_for_prediction_processed)\n",
    "shap.force_plot(explainer.expected_value, shap_values, data_for_prediction_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1682
    },
    "colab_type": "code",
    "id": "09Hpoh9G6Yu0",
    "outputId": "a38fbd49-ab9d-45f9-e669-04d9b8c3c8e9"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,30))\n",
    "pd.Series(shap_values[0], X_test.columns).sort_values().plot.barh(color='grey');"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "making-explaining-decisions_LIVE_LESSON.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
